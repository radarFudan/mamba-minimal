{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e80dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531467a2-5160-4073-a990-0d81d574b014",
   "metadata": {},
   "source": [
    "## (1) Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9337043-4e7a-4b20-9d89-6c6257245334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiops/wangsd/miniforge3/envs/env_mamba/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from model import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "# pretrained_model_name = 'state-spaces/mamba-370m'\n",
    "pretrained_model_name = 'state-spaces/mamba-2.8b'\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2efb17-37ad-472b-b029-9567acf17629",
   "metadata": {},
   "source": [
    "## (2) Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b2d62d-0d95-4a3f-bd98-aa37e3f26b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "            #  n_tokens_to_gen: int = 50,\n",
    "             n_tokens_to_gen: int = 256,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "    \n",
    "    for token_n in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            next_token_logits = model(indices_to_input)[:, -1]\n",
    "        \n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "        \n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "    \n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee877143-2042-4579-8042-a96db6200517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba is the first company outside of the major tech giants to create a smartphone that isn’t available exclusively through carrier partners. But like the Moto X Pure Edition, even though it isn’t available from carriers, the Mamba will work on any T-mobile network.\n",
      "\n",
      "Mamba’s features\n",
      "\n",
      "The Mamba features two-way communication capabilities that allow it to serve as an extension of your phone. As long as your phone is connected to a Wi-Fi network, the Mamba will alert you to when you are approaching the border of the zone for which it’s registered. If its proximity to a border goes undetected by the Mamba, you will receive a notification warning you of the impending zone.\n",
      "\n",
      "Mamba will also let you receive notifications on the T-mobile network if a nearby phone has entered the zone — even if its mobile carrier is different from your phone company.\n",
      "\n",
      "The Mamba will also let you call, text, and view any apps currently installed on any phone with a Wi-Fi signal in the same room. It will let users see incoming and outgoing phone calls, texts, and Mamba notifications on their nearby devices.\n",
      "\n",
      "Finally, it’s like a Google Fit tracker, showing users how and when\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'Mamba is the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d70549-597f-49ca-9185-2184d2576f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John: Hi!\n",
      "Sally: Hi!\n",
      "\n",
      "[John walks right past Sally into her car, leaving a huge pile of garbage in his wake]\n",
      "\n",
      "John: You've gotta be kidding me! The hell was in ALL OF THESE TINY PLASTIC SACKS?\n",
      "\n",
      "[SALLY GETS IN HER CAR. LOOKS LIKE SHE PLUSHED OUT IN FRONT OF IT!]\n",
      "\n",
      "Sally: John, what's going on?\n",
      "\n",
      "John: Don't you know? Every day is the same around here.\n",
      "Sally: Well if that's the case I need an extra 2-hour lunch. I didn't get one today. And as of right now, I'm outta paper towels.\n",
      "\n",
      "[SALLY DUSTS OFF A TOILET SEAT WITH A WHIFF OF BACON SAUCE]\n",
      "\n",
      "John: Did you just fart??\n",
      "\n",
      "Sally: Well, that depends. Are you saying it right?\n",
      "John: That's the saddest thing I've ever heard. You need to get laid.\n",
      "Sally: And you need to STFU.<|endoftext|><?php\n",
      "\n",
      "namespace Intervention\\Image;\n",
      "\n",
      "use Illuminate\\Support\\ServiceProvider;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'John: Hi!\\nSally:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d419fc9-066b-4818-812c-2f1952528bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is icing! So I don’t need a therapist for THAT.\n",
      "\n",
      "So, the truth is that life is the greatest ice-cream there is.\n",
      "\n",
      "I had a moment of revelation.\n",
      "\n",
      "And I’m here to tell you that you need to give yourself permission to say no.\n",
      "\n",
      "I want you to consider this:\n",
      "\n",
      "“I’ll make my own cake!”\n",
      "\n",
      "“I’ll make my own cookies!”\n",
      "\n",
      "“I’ll make my own ice-cream!”\n",
      "\n",
      "Let me just say right now that nobody’s making their own ice-cream, because it’s not that easy.\n",
      "\n",
      "Unless you’re a big boy or a big girl who can buy an ice-cream makers and a whole bunch of special ingredients, you won’t have the tools to create your very own treat.\n",
      "\n",
      "So if you really really really want to make something special for yourself, ask your parents. You will not get it approved because “you” are still too young. It will be expensive, you will have to make it on your own and probably they won’t like it.\n",
      "\n",
      "So then why would you ever agree to take this out of reach thing seriously?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'The meaning of life is '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b189e6e-6a96-4770-88cf-7c5de22cb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reverse_string(string) do\n",
      "  IO.inspect string\n",
      "end\n",
      "\n",
      "Output:\n",
      "\"hello world\"\n",
      "\"olleh woerld\"\n",
      "\n",
      "There is a function reverse in a module String, that can reverse string by character. That is why IO.inspect reverse_string(string) return you the result you were expecting.\n",
      "So, why doesn't IO.inspect reverse_string(\"hello world\") return a result you were expecting? The problem is that that particular version of IO.inspect, doesn't allow you to reverse string, that's why I'm getting the output:\n",
      "IO.inspect reverse_string(\"hello world\")\n",
      "# String\n",
      "\n",
      "But when you run IO.inspect(reverse_string(\"hello world\")) you get:\n",
      "IO.inspect reverse_string(\"hello world\")\n",
      "# \"hello world\"\n",
      "\n",
      "I'm not sure, what is even output by: IO.inspect reverse_string(\"hello world\") in the first case, but I don't think you'll be able to reverse a string any other way, than writing reverse_string(string) yourself.\n",
      "Also, you can use inspect method in conjunction with String.reverse.\n",
      "def reverse_string(string\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'def reverse_string('))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3afb51-5093-4c64-ac3f-43c2e6b20b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531acc0-b18f-472a-8e99-cee64dd51cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efe197-891a-4ab8-8cea-413d1fb1acda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99509b-df7b-4bac-b6a2-669f601ec1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
