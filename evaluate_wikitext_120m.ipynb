{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pretrained model I'm using is: state-spaces/mamba-790m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f60c3a2c7f4dc2a46c832d624f9042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a07a1e81d34622bf110b64430d0a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiops/wangsd/miniforge3/envs/env_mamba/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "[*] Creating wikitext-103 Dataset\n",
      "loader_name is val, ppl_over_loaders is [139.7777302648935, 77.87176517564225, 49.15296807324841, 35.92156886417661, 29.673878463686194, 26.759916905323472, 25.512165196557973, 24.873523371930595, 24.655187453214698, 25.111255620856603, 31.407989949322545, 79.33681365279371]\n",
      "loader_name is test, ppl_over_loaders is [139.84366749627696, 77.13766870371919, 48.4299414388772, 35.29418653114845, 29.17634450252662, 26.310642725285618, 25.007266227870147, 24.37497590900105, 24.156721249503253, 24.630436824077655, 30.78681211981892, 79.74084877967272]\n",
      "loader_name is train, ppl_over_loaders is []\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "from model import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from S5.dataloading import create_wikitext_dataset\n",
    "\n",
    "pretrained_model_name_list = [\"state-spaces/mamba-2.8b-slimpj\", \"state-spaces/mamba-2.8b\", \"state-spaces/mamba-1.4b\", \"state-spaces/mamba-790m\", \"state-spaces/mamba-370m\", \"state-spaces/mamba-130m\"]\n",
    "# pretrained_model_name = pretrained_model_name_list[-1]\n",
    "# pretrained_model_name = pretrained_model_name_list[-2]\n",
    "pretrained_model_name = pretrained_model_name_list[-3]\n",
    "print(\"The pretrained model I'm using is:\", pretrained_model_name)\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "\n",
    "# Get the data\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "# text = dataset[\"train\"][3]\n",
    "\n",
    "loader_names = [\"val\", \"test\", \"train\"]\n",
    "ppl_over_loaders = {\"val\":[], \"test\":[], \"train\":[]}\n",
    "\n",
    "# l_max_power_range = 16 # 120M\n",
    "l_max_power_range = 15 # 370M or 790M\n",
    "# l_max_power_range = 14 # up to 2.8B\n",
    "\n",
    "for l_max_power in range(4, l_max_power_range+1): # 16->32768\n",
    "    l_max = 2 ** l_max_power\n",
    "\n",
    "    config = {\n",
    "        \"l_max\": l_max, \n",
    "        \"data_dir\": \"./\",\n",
    "        \"batch_size\": int(2**l_max_power_range / l_max),\n",
    "        \"batch_size_eval\": int(2**l_max_power_range / l_max), \n",
    "        \"num_workers\": 4, \n",
    "        \"pin_memory\": False, \n",
    "        \"tokenizer\": \"EleutherAI/gpt-neox-20b\",\n",
    "    }\n",
    "    assert config[\"batch_size\"] > 0, \"batch_size must be positive\"\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_wikitext_dataset(config)\n",
    "\n",
    "    # Evaluate the perplexity\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Plan\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    loaders = [val_loader, test_loader, train_loader]\n",
    "\n",
    "    for loader, loader_name in zip(loaders, loader_names):\n",
    "        if loader_name == \"train\":\n",
    "            continue\n",
    "        # print(\"loader_name is \", loader_name)\n",
    "        # Iterate through the test data loader\n",
    "        for batch in loader:\n",
    "            input_ids = batch[0].to(device)  # Move input to GPU\n",
    "            output_ids = batch[1].to(device)  # Move output to GPU\n",
    "            \n",
    "            # Forward pass to get the logits\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids)\n",
    "            \n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), output_ids.view(-1), reduction='sum')\n",
    "            \n",
    "            # Update the total loss and token count\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += input_ids.numel()\n",
    "\n",
    "            perplexity = np.exp(total_loss / total_tokens)\n",
    "        ppl_over_loaders[loader_name].append(perplexity)\n",
    "\n",
    "            # print(f\"Running l_max = {l_max}\\n  perplexity: {perplexity:.2f}\\n  total loss is {total_loss}\\n  total tokens is {total_tokens}\")\n",
    "for loader_name in loader_names:\n",
    "    print(f\"loader_name is {loader_name}, ppl_over_loaders is {ppl_over_loaders[loader_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# store the data into csv\n",
    "\n",
    "df = pd.DataFrame(ppl_over_loaders, columns = ['val', 'test'])\n",
    "df[\"length\"] = [2 ** aha for aha in range(4, l_max_power_range+1)]\n",
    "# print(f\"ppl_wikitext_{pretrained_model_name[13:]}.csv\")\n",
    "df.to_csv(f\"ppl_wikitext_{pretrained_model_name[13:]}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
