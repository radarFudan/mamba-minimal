{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_supported_precision(training: bool, tpu: bool = False) -> str:\n",
    "    \"\"\"Return default precision that is supported by the hardware.\n",
    "\n",
    "    Args:\n",
    "        training: `-mixed` or `-true` version of the precision to use\n",
    "        tpu: whether TPU device is used\n",
    "\n",
    "    Returns:\n",
    "        default precision that is suitable for the task and is supported by the hardware\n",
    "    \"\"\"\n",
    "    if tpu:\n",
    "        return \"32-true\"\n",
    "    if not torch.cuda.is_available() or torch.cuda.is_bf16_supported():\n",
    "        return \"bf16-mixed\" if training else \"bf16-true\"\n",
    "    return \"16-mixed\" if training else \"16-true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "strategy=\"auto\"\n",
    "tpu=False\n",
    "precision = None or get_default_supported_precision(training=True, tpu=tpu)\n",
    "\n",
    "fabric = L.Fabric(devices=1, strategy=strategy, precision=precision, loggers=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "import random\n",
    "\n",
    "from TinyLlama.lit_gpt.packed_dataset import CombinedDataset, PackedDataset\n",
    "# from TinyLlama.pretrain.tinyllama import create_dataloaders\n",
    "\n",
    "train_data_config = [\n",
    "    (\"train_ind\", 1.0),\n",
    "]\n",
    "\n",
    "val_data_config = [\n",
    "    (\"train_ind\", 1.0),\n",
    "]\n",
    "\n",
    "def create_dataloader(\n",
    "    batch_size: int, block_size: int, data_dir: Path, fabric, shuffle: bool = True, seed: int = 12345, split=\"train\"\n",
    ") -> DataLoader:\n",
    "    datasets = []\n",
    "    data_config = train_data_config if split == \"train\" else val_data_config\n",
    "    for prefix, _ in data_config:\n",
    "        filenames = sorted(glob.glob(str(data_dir / f\"{prefix}*\")))\n",
    "        random.seed(seed)\n",
    "        random.shuffle(filenames)\n",
    "\n",
    "        dataset = PackedDataset(\n",
    "            filenames,\n",
    "            # n_chunks control the buffer size. \n",
    "            # Note that the buffer size also impacts the random shuffle\n",
    "            # (PackedDataset is an IterableDataset. So the shuffle is done by prefetch a buffer and shuffle the buffer)\n",
    "            n_chunks=8,\n",
    "            block_size=block_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed+fabric.global_rank,\n",
    "            num_processes=fabric.world_size,\n",
    "            process_rank=fabric.global_rank,\n",
    "        )\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    if not datasets:\n",
    "        raise RuntimeError(\n",
    "            f\"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset.\"\n",
    "        )\n",
    "\n",
    "    weights = [weight for _, weight in data_config]\n",
    "    sum_weights = sum(weights)\n",
    "    weights = [el / sum_weights for el in weights]\n",
    "\n",
    "    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)\n",
    "\n",
    "    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    batch_size: int,\n",
    "    block_size: int,\n",
    "    fabric,\n",
    "    train_data_dir: Path = Path(\"data/redpajama_sample\"),\n",
    "    val_data_dir: Optional[Path] = None,\n",
    "    seed: int = 12345,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    # Increase by one because we need the next word as well\n",
    "    effective_block_size = block_size + 1\n",
    "    train_dataloader = create_dataloader(\n",
    "        batch_size=batch_size,\n",
    "        block_size=effective_block_size,\n",
    "        fabric=fabric,\n",
    "        data_dir=train_data_dir,\n",
    "        shuffle=True,\n",
    "        seed=seed,\n",
    "        split=\"train\"\n",
    "    )\n",
    "    val_dataloader = (\n",
    "        create_dataloader(\n",
    "            batch_size=batch_size,\n",
    "            block_size=effective_block_size,\n",
    "            fabric=fabric,\n",
    "            data_dir=val_data_dir,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            split=\"validation\"\n",
    "        )\n",
    "        if val_data_dir\n",
    "        else None\n",
    "    )\n",
    "    return train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pretrained model I'm using is: state-spaces/mamba-370m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiops/wangsd/miniforge3/envs/env_mamba/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader_name is val, ppl_over_loaders is [38.365222204445494, 25.992814713117472, 20.00329615886128, 17.047069564983918, 11.035977720353646, 10.925790582248762, 8.88301029536941, 7.1713857470179105, 8.41482932120078, 9.128650253042753, 14.138112667928835, 207.5466573880326]\n",
      "loader_name is train, ppl_over_loaders is []\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "from model import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from S5.dataloading import create_wikitext_dataset\n",
    "\n",
    "\n",
    "pretrained_model_name_list = [\"state-spaces/mamba-2.8b-slimpj\", \"state-spaces/mamba-2.8b\", \"state-spaces/mamba-1.4b\", \"state-spaces/mamba-790m\", \"state-spaces/mamba-370m\", \"state-spaces/mamba-130m\"]\n",
    "pretrained_model_name = pretrained_model_name_list[-2]\n",
    "print(\"The pretrained model I'm using is:\", pretrained_model_name)\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "\n",
    "loader_names = [\"val\", \"train\"]\n",
    "ppl_over_loaders = {\"val\":[], \"train\":[]}\n",
    "\n",
    "# l_max_power_range = 16 # 120M\n",
    "l_max_power_range = 15 # 370M or 790M\n",
    "# l_max_power_range = 14 # up to 2.8B\n",
    "\n",
    "for l_max_power in range(4, l_max_power_range+1): # 16->32768\n",
    "    l_max = 2 ** l_max_power\n",
    "\n",
    "    config = {\n",
    "        \"l_max\": l_max, \n",
    "        \"data_dir\": \"./\",\n",
    "        \"batch_size\": int(2**l_max_power_range / l_max),\n",
    "        \"batch_size_eval\": int(2**l_max_power_range / l_max), \n",
    "        \"num_workers\": 4, \n",
    "        \"pin_memory\": False, \n",
    "        \"tokenizer\": \"EleutherAI/gpt-neox-20b\",\n",
    "        \"train_data_dir\": Path(\"/home/aiops/wangsd/TinyLlama/data/the_pile_deduplicated_EleutherAI_combined\"),\n",
    "        \"val_data_dir\": Path(\"/home/aiops/wangsd/TinyLlama/data/the_pile_deduplicated_EleutherAI_combined\"),\n",
    "    }\n",
    "    assert config[\"batch_size\"] > 0, \"batch_size must be positive\"\n",
    "\n",
    "    # train_loader, val_loader, test_loader = create_wikitext_dataset(config)\n",
    "\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        block_size=config[\"l_max\"],\n",
    "        fabric=fabric,\n",
    "        train_data_dir=config[\"train_data_dir\"],\n",
    "        val_data_dir=config[\"val_data_dir\"],\n",
    "        seed=3412,\n",
    "    )\n",
    "\n",
    "    # Evaluate the perplexity\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Plan\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    loaders = [val_loader, train_loader]\n",
    "\n",
    "    for loader, loader_name in zip(loaders, loader_names):\n",
    "        if loader_name == \"train\":\n",
    "            continue\n",
    "        # print(\"loader_name is \", loader_name)\n",
    "        # Iterate through the test data loader\n",
    "        for batch_index, batch in enumerate(loader):\n",
    "            input_ids = batch[:,:-1].to(device)  # Move input to GPU\n",
    "            output_ids = batch[:,1:].to(device)  # Move output to GPU\n",
    "\n",
    "            # print(f\"input_ids.shape is {input_ids.shape}\")\n",
    "            # print(f\"output_ids.shape is {output_ids.shape}\")\n",
    "            \n",
    "            # Forward pass to get the logits\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids)\n",
    "            \n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), output_ids.view(-1), reduction='sum')\n",
    "            \n",
    "            # Update the total loss and token count\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += input_ids.numel()\n",
    "\n",
    "            perplexity = np.exp(total_loss / total_tokens)\n",
    "\n",
    "            if batch_index > 16:\n",
    "                break\n",
    "        ppl_over_loaders[loader_name].append(perplexity)\n",
    "\n",
    "            # print(f\"Running l_max = {l_max}\\n  perplexity: {perplexity:.2f}\\n  total loss is {total_loss}\\n  total tokens is {total_tokens}\")\n",
    "for loader_name in loader_names:\n",
    "    print(f\"loader_name is {loader_name}, ppl_over_loaders is {ppl_over_loaders[loader_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# store the data into csv\n",
    "\n",
    "df = pd.DataFrame(ppl_over_loaders, columns = ['val'])\n",
    "df[\"length\"] = [2 ** aha for aha in range(4, l_max_power_range+1)]\n",
    "df.to_csv(f\"ppl_pile_{pretrained_model_name[13:]}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
